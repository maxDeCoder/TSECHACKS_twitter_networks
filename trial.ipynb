{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'jigsaw_train_hateoffensive.csv'\n",
    "df = pd.read_csv(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    new_words = []\n",
    "    for word in text:\n",
    "        w = re.sub(r'[^\\w\\s]','',word) #remove everything except words and space\n",
    "        w = re.sub(r'_','',w) #how to remove underscore as well\n",
    "        new_words.append(w)\n",
    "    return \"\".join(new_words)\n",
    "\n",
    "def remove_extras(text):\n",
    "    text = remove_punct(text)\n",
    "    text = re.sub(\"\\n+\", \"\", text)\n",
    "    text = re.sub(\"\\s+$\", \"\", text)\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    text = re.sub(f\"http\\S+\", \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_num = {\n",
    "    \"neutral\": 0,\n",
    "    \"offensive\": 1,\n",
    "    \"hate_speech\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df[\"text\"].map(remove_extras), \n",
    "texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[df.columns[3:]]\n",
    "labels = tf.keras.utils.to_categorical(labels[\"hateoffensive_class\"].map(lambda x: label_to_num[x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 512\n",
    "num_samples = len(texts)\n",
    "\n",
    "Xids = np.zeros((num_samples, seq_len))\n",
    "Xmask = np.zeros((num_samples, seq_len))\n",
    "# labels = np.expand_dims(, axis=0).T\n",
    "\n",
    "Xids.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "for i, phrase in enumerate(tqdm(texts)):\n",
    "    token = tokenizer.encode_plus(\n",
    "        phrase, max_length=seq_len, add_special_tokens=True, \n",
    "        padding=\"max_length\", truncation=True, return_tensors='tf')\n",
    "\n",
    "    Xids[i, :] = token['input_ids']\n",
    "    Xmask[i, :] = token['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(inputs_ids, masks, labels):\n",
    "    return {\n",
    "        'input_ids': inputs_ids,\n",
    "        'attention_mask': masks\n",
    "    }, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))\n",
    "dataset = dataset.map(map_func)\n",
    "dataset = dataset.shuffle(buffer_size=1000).batch(batch_size, drop_remainder=True)\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.9\n",
    "size = int((num_samples/batch_size) * split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset.take(size)\n",
    "val_ds = dataset.skip(size)\n",
    "\n",
    "# del [dataset, Xids, Xmask, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "bert = TFAutoModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "# make untrainable\n",
    "bert.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = keras.layers.Input(shape=(seq_len,), name=\"input_ids\", dtype=\"int32\")\n",
    "attention_mask = keras.layers.Input(shape=(seq_len,), name=\"attention_mask\", dtype=\"int32\")\n",
    "\n",
    "embeddings = bert.bert(input_ids, attention_mask=attention_mask)[1]\n",
    "\n",
    "# x = layers.Dense(1024, activation=\"relu\")(embeddings)\n",
    "# x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(3, activation=\"softmax\")(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=[input_ids, attention_mask], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = keras.optimizers.Adam(),\n",
    "    loss = \"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_bert_embedding(texts):\n",
    "\n",
    "    if type(texts) == str:\n",
    "        texts = [texts]\n",
    "\n",
    "    num_samples = len(texts)\n",
    "\n",
    "    Xids = np.zeros((num_samples, seq_len))\n",
    "    Xmask = np.zeros((num_samples, seq_len))\n",
    "\n",
    "    for i, phrase in enumerate(texts):\n",
    "        token = tokenizer.encode_plus(\n",
    "        phrase, max_length=seq_len, add_special_tokens=True, \n",
    "        padding=\"max_length\", truncation=True, return_tensors='tf')\n",
    "\n",
    "    Xids[i, :] = token['input_ids']\n",
    "    Xmask[i, :] = token['attention_mask']\n",
    "\n",
    "    return Xids, Xmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, mask = get_bert_embedding(\"There is such a violent earthquake here in my city of Berlin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "### from models.py\n",
    "from models import *\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two\")\n",
    "model = Model_Rational_Label.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two\")\n",
    "inputs = tokenizer('He is a great guy', return_tensors=\"pt\")\n",
    "prediction_logits, _ = model(input_ids=inputs['input_ids'],attention_mask=inputs['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "063da5ec96525b703b2b4b88ba5015678e29341c0c783b18b72decb99d23a1d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
